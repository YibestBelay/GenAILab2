{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed5bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de681454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = BertModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=True\n",
    ")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5901162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The mechanic inspected the engine because it was noisy.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc10829",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b057c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6518d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "attentions = outputs.attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57724e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "head = 0\n",
    "\n",
    "attention_matrix = attentions[layer][0, head].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd865b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap=\"viridis\")\n",
    "\n",
    "plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "plt.yticks(range(len(tokens)), tokens)\n",
    "\n",
    "plt.colorbar()\n",
    "plt.title(\"Self-Attention Heatmap (Layer 1, Head 0)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"outputs/attention_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f893e39d",
   "metadata": {},
   "source": [
    "A2 — Understand Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_original = \"The cat sat on the mat\"\n",
    "sentence_scrambled = \"Mat the on sat cat the\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Last hidden state: (batch, seq_len, hidden_dim)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Mean pooling over tokens\n",
    "    sentence_embedding = token_embeddings.mean(dim=1)\n",
    "\n",
    "    return sentence_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201735b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_original = get_sentence_embedding(sentence_original)\n",
    "emb_scrambled = get_sentence_embedding(sentence_scrambled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7efc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_similarity = F.cosine_similarity(\n",
    "    emb_original, emb_scrambled\n",
    ")\n",
    "\n",
    "sentence_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    return outputs.last_hidden_state.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829648da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_original = get_token_embeddings(sentence_original)\n",
    "tokens_scrambled = get_token_embeddings(sentence_scrambled)\n",
    "\n",
    "token_level_similarity = F.cosine_similarity(\n",
    "    tokens_original.mean(dim=0),\n",
    "    tokens_scrambled.mean(dim=0),\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "token_level_similarity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680622a",
   "metadata": {},
   "source": [
    "A3 — Encoder vs Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2ccc5",
   "metadata": {},
   "source": [
    "A3.1 — Encoder Task: Fill-Mask (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transformers are [MASK] at understanding context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fill_mask(text)\n",
    "\n",
    "for r in results[:5]:\n",
    "    print(f\"{r['token_str']:>12}  |  score = {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1d1ce",
   "metadata": {},
   "source": [
    "A3.2 — Decoder Task: Text Generation (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "gpt_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c72f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Transformers are powerful because\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df32d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = gpt_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = gpt_model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "for i, out in enumerate(outputs, 1):\n",
    "    print(f\"\\nOutput {i}:\")\n",
    "    print(gpt_tokenizer.decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6437b",
   "metadata": {},
   "source": [
    "B — Tokens, Embeddings & Context Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a487a1a",
   "metadata": {},
   "source": [
    "B1) Tokenization Strategies: Word vs Subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "306c42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91daa40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    \"unbelievable\",\n",
    "    \"internationalization\",\n",
    "    \"electroencephalography\",\n",
    "    \"bioinformatics\",\n",
    "    \"mecahnical\"   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ac3769d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>BERT Tokens</th>\n",
       "      <th>BERT Token Count</th>\n",
       "      <th>GPT-2 Tokens</th>\n",
       "      <th>GPT-2 Token Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unbelievable</td>\n",
       "      <td>[unbelievable]</td>\n",
       "      <td>1</td>\n",
       "      <td>[un, bel, iev, able]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>internationalization</td>\n",
       "      <td>[international, ##ization]</td>\n",
       "      <td>2</td>\n",
       "      <td>[international, ization]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electroencephalography</td>\n",
       "      <td>[electro, ##ence, ##pha, ##log, ##raphy]</td>\n",
       "      <td>5</td>\n",
       "      <td>[elect, ro, ence, phal, ography]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bioinformatics</td>\n",
       "      <td>[bio, ##in, ##form, ##atics]</td>\n",
       "      <td>4</td>\n",
       "      <td>[b, io, in, format, ics]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mecahnical</td>\n",
       "      <td>[me, ##ca, ##hn, ##ical]</td>\n",
       "      <td>4</td>\n",
       "      <td>[m, ec, ahn, ical]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Word                               BERT Tokens  \\\n",
       "0            unbelievable                            [unbelievable]   \n",
       "1    internationalization                [international, ##ization]   \n",
       "2  electroencephalography  [electro, ##ence, ##pha, ##log, ##raphy]   \n",
       "3          bioinformatics              [bio, ##in, ##form, ##atics]   \n",
       "4              mecahnical                  [me, ##ca, ##hn, ##ical]   \n",
       "\n",
       "   BERT Token Count                      GPT-2 Tokens  GPT-2 Token Count  \n",
       "0                 1              [un, bel, iev, able]                  4  \n",
       "1                 2          [international, ization]                  2  \n",
       "2                 5  [elect, ro, ence, phal, ography]                  5  \n",
       "3                 4          [b, io, in, format, ics]                  5  \n",
       "4                 4                [m, ec, ahn, ical]                  4  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for word in words:\n",
    "    bert_tokens = bert_tokenizer.tokenize(word)\n",
    "    gpt2_tokens = gpt2_tokenizer.tokenize(word)\n",
    "\n",
    "    rows.append({\n",
    "        \"Word\": word,\n",
    "        \"BERT Tokens\": bert_tokens,\n",
    "        \"BERT Token Count\": len(bert_tokens),\n",
    "        \"GPT-2 Tokens\": gpt2_tokens,\n",
    "        \"GPT-2 Token Count\": len(gpt2_tokens),\n",
    "    })\n",
    "\n",
    "token_table = pd.DataFrame(rows)\n",
    "token_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
